//CNN forward pass based on 
// http://cs231n.github.io/convolutional-networks/


affine CNN [W,H,D]->{:}
	
	locals
		tmp1 :
	let
		(tmp1) = CONVLayer[W,H,D,10,20](tmp1);
.

//CONV layer that takes:
// W : width of the image
// H : height of the image
// D : depth of the image (color)
// F : number of filters in this layer

affine CONVLayer [W,H,D,F,X,Y]->{:}
	inputs
		image : [W,H,D]
		weights : [W,H,D,F]
	outputs
		out : [W,H,F]
	let
		out[i,j,k] = conv({[x,y,z]: -X<=x<=X and -Y<=y<=Y and 0<=z<D}, weights[x,y,z,k], image[i+x,j+y,k+z]);
		out[i,j,k] = conv( [X,Y,D] as [x,y,z], weights[x,y,z,k], image[i+x,j+y,k+z]);
.


//RELU layer that just applies the activation function
//(RELU comes from REcitified Linear Unit or max(0,x))
external activation(1)
affine RELULayer [W,H,D]->{:}
	inputs
		image : { [i,j,k] -> [i,i,i] : } @ [W,H,D]
	outputs
		out : [W,H,D]
	let
		out = activation(image);
.

//Sampling layer
// Sampling factor must be a constant to stay polyhedral
constant factor=2
affine POOLLayer [W,H,D,WW,HH]->{:factor*WW=W and factor*HH=H}
	inputs
		image : [W,H,D]
	outputs
		out : [WW,HH,D]
	let
		out[i,j,k] = reduce(max, [i',j'], {:factor*i<=i'<factor*(i+1) and factor*j<=j'<factor*(j+1)} : image[i',j',k]);
.

//Fully connected layer
// same thing as an ANNForwardLayer, except for the dimensionality of the input data
affine FCLayer [W,H,D,O]-> {:}
	inputs
		image : [W,H,D]
		weights : [W,H,D,O]
	outputs
		out : [O]
	let
		out[x] = reduce(+, [i,j,k], weights[i,j,k,x]*image[i,j,k]);
.







